# Pipeline Datanex - Wiki Processor

Pipeline de nivel producciÃ³n para descargar, procesar y unificar la wiki de Datanex (base de datos del Hospital ClÃ­nic) con scraping responsable, trazabilidad completa y reproducibilidad cientÃ­fica.

> **ğŸ“– Para documentaciÃ³n detallada del sistema de scraping**, ver [SCRAPING_GUIDE.md](SCRAPING_GUIDE.md)

## ğŸ“‹ DescripciÃ³n

Este proyecto descarga todas las pÃ¡ginas de la wiki de Datanex desde GitLab, las procesa, filtra las pÃ¡ginas relevantes, las convierte a Markdown y genera un archivo unificado (`vibe_SQL_copilot.txt`) que combina un prompt personalizado con toda la documentaciÃ³n de la estructura de la base de datos. Este archivo estÃ¡ diseÃ±ado para ser usado como contexto en LLMs (GitHub Copilot, Claude, ChatGPT, etc.) para generar queries SQL de alta calidad.

> **Nota**: Esta versiÃ³n genera un documento de contexto **ligero** (~600 lÃ­neas) que incluye solo la documentaciÃ³n de la wiki, sin los diccionarios de datos (que aÃ±adÃ­an ~38,000 lÃ­neas adicionales). Esto hace el archivo mÃ¡s manejable para la mayorÃ­a de LLMs.

## ğŸš€ CaracterÃ­sticas

### Scraping de Nivel ProducciÃ³n
- **Scraping responsable**: Rate limiting configurable (default: 2s entre requests)
- **Reintentos automÃ¡ticos**: Backoff exponencial con hasta 3 intentos por pÃ¡gina
- **ValidaciÃ³n de integridad**: Checksums SHA256 de cada pÃ¡gina descargada
- **DetecciÃ³n de cambios**: Solo re-descarga pÃ¡ginas modificadas (ahorra tiempo y ancho de banda)
- **Logging estructurado**: Logs JSON con timestamp, URL, checksums y resultados
- **Metadatos completos**: Manifest, logs y checksums para auditorÃ­a y reproducibilidad
- **Manejo robusto de errores**: ContinÃºa ante fallos individuales, registra todo
- **Dominio seguro**: Solo sigue enlaces dentro del wiki (no sale del dominio)
- **User-Agent explÃ­cito**: IdentificaciÃ³n clara como archivador clÃ­nico/investigaciÃ³n

### Pipeline de Procesamiento
- **Descarga automÃ¡tica**: Descarga recursiva de todas las pÃ¡ginas del wiki de GitLab
- **Estructura jerÃ¡rquica**: Mantiene organizaciÃ³n de carpetas reflejando la wiki
- **Filtrado inteligente**: Excluye pÃ¡ginas no relevantes segÃºn configuraciÃ³n
- **ConversiÃ³n a Markdown**: HTML â†’ Markdown preservando tablas y estructura
- **Limpieza automÃ¡tica**: Elimina secciones no relevantes y normaliza formato
- **UnificaciÃ³n**: Combina todos los documentos en un solo archivo optimizado
- **Output ligero**: Genera un archivo de contexto compacto (~600 lÃ­neas) ideal para LLMs
- **Pipeline modular**: Cada paso es independiente, testeable y auditable

## ğŸ“ Estructura del Proyecto

```
pipeline_datanex/
â”œâ”€â”€ src/                          # CÃ³digo fuente
â”‚   â”œâ”€â”€ download_wiki.py          # Descarga de pÃ¡ginas wiki
â”‚   â”œâ”€â”€ extract_text.py           # ExtracciÃ³n a Markdown
â”‚   â”œâ”€â”€ unify_markdown.py         # UnificaciÃ³n de markdowns
â”‚   â””â”€â”€ create_final_output.py    # CreaciÃ³n del archivo final
â”œâ”€â”€ test/                         # Tests/Pasos del pipeline
â”‚   â”œâ”€â”€ test_download_wiki.py
â”‚   â”œâ”€â”€ test_filter_useful_pages.py
â”‚   â”œâ”€â”€ test_extract_text.py
â”‚   â”œâ”€â”€ test_download_linked_pages.py
â”‚   â”œâ”€â”€ test_unify_markdown.py
â”‚   â”œâ”€â”€ test_create_final_output.py
â”‚   â””â”€â”€ run_all_tests.py          # Ejecuta todo el pipeline
â”œâ”€â”€ data/                         # Datos procesados (ignorado en git)
â”‚   â”œâ”€â”€ wiki_html/                # HTML descargado (con estructura jerÃ¡rquica)
â”‚   â”‚   â”œâ”€â”€ metadata/             # Metadatos de descarga (manifest, logs, checksums)
â”‚   â”‚   â”œâ”€â”€ home.html
â”‚   â”‚   â”œâ”€â”€ datanex/              # Subcarpetas segÃºn estructura del wiki
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ wiki_work_html/           # HTML filtrado (pÃ¡ginas Ãºtiles)
â”‚   â”œâ”€â”€ wiki_markdown/            # Markdowns generados
â”‚   â””â”€â”€ wiki_unified.md           # Markdown unificado
â”œâ”€â”€ main.py                       # Script principal (pipeline de 5 pasos)
â”œâ”€â”€ ejecutar_pipeline.bat         # Script batch para ejecutar en Windows
â”œâ”€â”€ ejecutar_pipeline.sh          # Script bash para ejecutar en Linux/Mac
â”œâ”€â”€ prompt.txt                    # Prompt para el LLM
â”œâ”€â”€ pags_descarte.txt             # Lista de pÃ¡ginas a descartar/excluir
â”œâ”€â”€ README_vibe_query.md          # README estÃ¡tico para repo de salida
â””â”€â”€ vibe_SQL_copilot.txt          # Archivo final generado (~600 lÃ­neas)
```

## ğŸ”§ InstalaciÃ³n

### Requisitos

- Python 3.8 o superior
- pip

### Pasos

1. **Clonar el repositorio**:
```bash
git clone https://github.com/ramsestein/vibe_SQL_Datanex.git
cd vibe_SQL_Datanex
```

2. **Crear entorno virtual** (recomendado):
```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate
```

3. **Instalar dependencias**:
```bash
pip install -r requirements.txt
```

## ğŸ“¦ Dependencias

- `requests` - Para descargar pÃ¡ginas web
- `beautifulsoup4` - Para parsear HTML
- `markdownify` - Para convertir HTML a Markdown

## ğŸ¯ Uso

### Ejecutar el pipeline completo

**Windows (mÃ¡s fÃ¡cil)**:
```bash
ejecutar_pipeline.bat
```

**Linux/Mac (mÃ¡s fÃ¡cil)**:
```bash
./ejecutar_pipeline.sh
```

**Desde la lÃ­nea de comandos**:
```bash
python main.py
```

Ambos scripts (`ejecutar_pipeline.bat` y `ejecutar_pipeline.sh`) incluyen:
- CreaciÃ³n automÃ¡tica del entorno virtual si no existe
- VerificaciÃ³n e instalaciÃ³n automÃ¡tica de dependencias
- EjecuciÃ³n automÃ¡tica del pipeline completo
- **Push automÃ¡tico**: Al finalizar el pipeline, sube automÃ¡ticamente los siguientes archivos al repositorio [vibe_query_DataNex](https://github.com/ramsestein/vibe_query_DataNex):
  - `vibe_SQL_copilot.txt` - El archivo principal con toda la documentaciÃ³n
  - `README.md` - Instrucciones de uso (generado desde `README_vibe_query.md`)

Este comando ejecuta todos los pasos del pipeline en secuencia:

1. **Descarga desde home**: Descarga la pÃ¡gina "home" de la wiki que contiene el menÃº lateral con todas las pÃ¡ginas disponibles
2. **Filtrado**: Excluye las pÃ¡ginas listadas en `pags_descarte.txt` (procesa todas las demÃ¡s)
3. **ExtracciÃ³n a Markdown**: Convierte las pÃ¡ginas Ãºtiles a Markdown
4. **UnificaciÃ³n de markdowns**: Combina todos los markdowns de la wiki en un solo archivo (excluyendo las de `pags_descarte.txt`)
5. **Archivo final**: Combina `prompt.txt` + `wiki_unified.md` â†’ `vibe_SQL_copilot.txt`

### Ejecutar pasos individuales

Cada paso puede ejecutarse de forma independiente usando los scripts de test:

```bash
# Paso 1: Descarga
python test/test_download_wiki.py

# Paso 2: Filtrado
python test/test_filter_useful_pages.py

# Paso 3: ExtracciÃ³n
python test/test_extract_text.py

# Paso 4: UnificaciÃ³n de markdowns
python test/test_unify_markdown.py

# Paso 5: Archivo final
python test/test_create_final_output.py
```

### Ejecutar todos los pasos en secuencia

```bash
python test/run_all_tests.py
```

## âš™ï¸ ConfiguraciÃ³n

### Archivo `pags_descarte.txt`

Este archivo define quÃ© pÃ¡ginas de la wiki se **EXCLUIRÃN** (descartarÃ¡n) del prompt final. Cada lÃ­nea debe contener el nombre de la pÃ¡gina a excluir (sin extensiÃ³n):

```
News
FAQs
Access-Instructions
...
```

**âš ï¸ Importante**: 
- La pÃ¡gina `Overview` **siempre se incluirÃ¡**, incluso si estÃ¡ en esta lista de exclusiÃ³n.
- Si el archivo estÃ¡ vacÃ­o o no existe, se procesarÃ¡n **todas** las pÃ¡ginas disponibles.
- Esta es una lista de **exclusiÃ³n**, no de inclusiÃ³n.
- Las pÃ¡ginas listadas aquÃ­ tambiÃ©n se excluirÃ¡n automÃ¡ticamente durante la unificaciÃ³n de markdowns.

### Archivo `prompt.txt`

Este archivo contiene el prompt que se incluirÃ¡ al inicio del archivo final. Define el comportamiento esperado del LLM al usar el contexto. Debe contener la secciÃ³n `### CONTEXTO ###` donde se insertarÃ¡ la documentaciÃ³n de la wiki.

## ğŸ“Š Pipeline Detallado

### Paso 1: Descarga desde home (Scraping Responsable)
- Descarga la pÃ¡gina "home" de la wiki desde [GitLab](https://gitlab.com/dsc-clinic/datascope/-/wikis/home)
- Extrae todos los enlaces del sidebar/menÃº lateral y contenido principal
- Sigue recursivamente enlaces internos del wiki (no sale del dominio)
- **Rate limiting**: 2 segundos entre requests (configurable)
- **Reintentos**: Hasta 3 intentos con backoff exponencial (2, 4, 8 segundos)
- **ValidaciÃ³n**: Checksums SHA256, tamaÃ±o mÃ­nimo, Content-Type
- **DetecciÃ³n de cambios**: Solo re-descarga si el contenido cambiÃ³
- Guarda HTML en estructura jerÃ¡rquica: `data/wiki_html/` con subcarpetas
- **Metadatos completos**:
  - `metadata/manifest.json`: Inventario completo (timestamp, URLs, lista de pÃ¡ginas)
  - `metadata/download_log.jsonl`: Log estructurado de cada operaciÃ³n (append-only)
  - `metadata/page_checksums.json`: SHA256 de cada pÃ¡gina para detecciÃ³n de cambios
  - `metadata/README.md`: DocumentaciÃ³n de metadatos y reproducibilidad

### Paso 2: Filtrado de pÃ¡ginas Ãºtiles
- Lee `pags_descarte.txt` (lista de pÃ¡ginas a EXCLUIR/DESCARTAR)
- Copia TODAS las pÃ¡ginas de `data/wiki_html/` EXCEPTO las listadas en `pags_descarte.txt`
- Siempre incluye `Overview` aunque estÃ© en la lista de exclusiÃ³n
- Guarda las pÃ¡ginas filtradas en `data/wiki_work_html/`

### Paso 3: ExtracciÃ³n a Markdown de pÃ¡ginas Ãºtiles
- Convierte todas las pÃ¡ginas filtradas a Markdown
- Guarda en `data/wiki_markdown/`

### Paso 4: UnificaciÃ³n de markdowns
- Combina todos los markdowns en un solo archivo
- **Excluye automÃ¡ticamente** las pÃ¡ginas listadas en `pags_descarte.txt`
- Elimina secciones no relevantes (como "## Wiki Pages")
- Limpia saltos de lÃ­nea dobles
- Convierte tablas HTML a formato Markdown
- Guarda en `data/wiki_unified.md`

### Paso 5: Archivo final
- Combina `prompt.txt` + `wiki_unified.md`
- Inserta el contenido de la wiki despuÃ©s de `### CONTEXTO ###`
- Guarda en `vibe_SQL_copilot.txt` (~600 lÃ­neas, compacto y manejable para LLMs)

## ğŸ“ Archivos Generados

Durante la ejecuciÃ³n del pipeline se generan los siguientes archivos:

### Datos de Descarga
- `data/wiki_html/`: Archivos HTML descargados con estructura jerÃ¡rquica
  - `metadata/manifest.json`: Inventario completo de la descarga
  - `metadata/download_log.jsonl`: Log estructurado (append-only, mantiene histÃ³rico)
  - `metadata/page_checksums.json`: SHA256 checksums para validaciÃ³n
  - `metadata/README.md`: DocumentaciÃ³n de metadatos
  - `home.html`, `datanex/`, etc.: PÃ¡ginas organizadas en carpetas

### Datos Procesados
- `data/wiki_work_html/`: Archivos HTML filtrados (solo pÃ¡ginas Ãºtiles)
- `data/wiki_markdown/`: Archivos Markdown generados de cada pÃ¡gina
- `data/wiki_unified.md`: Markdown unificado con todo el contenido de la wiki

### Salida Final
- `vibe_SQL_copilot.txt`: Archivo final listo para usar con LLMs (~600 lÃ­neas):
  ```
  [Contenido de prompt.txt]
  ### CONTEXTO ###
  [Contenido de wiki_unified.md - documentaciÃ³n de tablas]
  ```

## ğŸ” Funciones Principales

### `download_wiki_pages()`
Descarga pÃ¡ginas wiki desde GitLab con scraping responsable de nivel producciÃ³n:
- Rate limiting configurable (default: 2s)
- Reintentos automÃ¡ticos con backoff exponencial
- ValidaciÃ³n de integridad (SHA256 checksums)
- DetecciÃ³n de cambios (no re-descarga sin modificaciones)
- Logging estructurado completo
- Metadatos de trazabilidad (manifest, logs, checksums)

### `download_linked_pages()`
Extrae enlaces de archivos Markdown y descarga las pÃ¡ginas referenciadas.

### `filter_useful_pages()`
Filtra pÃ¡ginas excluyendo las que estÃ¡n en `pags_descarte.txt`. Procesa todas las pÃ¡ginas disponibles excepto las listadas. La pÃ¡gina `Overview` siempre se incluye.

### `extract_text()`
Convierte HTML a Markdown preservando tablas y estructura.

### `unify_markdowns()`
Combina mÃºltiples archivos Markdown en uno solo, limpiando contenido no relevante.

### `create_final_output()`
Combina el prompt con la documentaciÃ³n unificada, insertando el contenido de la wiki en la secciÃ³n `### CONTEXTO ###`.

## ğŸ§ª Testing

Los archivos en `test/` actÃºan como pasos individuales del pipeline y pueden ejecutarse de forma independiente para debugging o para ejecutar solo una parte del proceso.

## ğŸ“„ Licencia

Este proyecto es de uso interno para el Hospital ClÃ­nic.

## ğŸ”¬ Reproducibilidad y Trazabilidad

Este pipeline estÃ¡ diseÃ±ado para entornos clÃ­nicos y de investigaciÃ³n donde la reproducibilidad y trazabilidad son **obligatorias**:

### Reproducibilidad
1. **Determinismo**: Misma entrada â†’ misma salida
2. **Versionado completo**: Todo el cÃ³digo estÃ¡ en Git
3. **Dependencias fijadas**: `requirements.txt` con versiones especÃ­ficas
4. **Metadatos timestamped**: Cada descarga registra fecha, hora y configuraciÃ³n
5. **ConfiguraciÃ³n explÃ­cita**: Todos los parÃ¡metros estÃ¡n documentados

### Trazabilidad
1. **Manifest completo** (`manifest.json`): QuÃ© se descargÃ³, cuÃ¡ndo, desde dÃ³nde
2. **Log estructurado** (`download_log.jsonl`): Cada operaciÃ³n registrada con:
   - Timestamp ISO 8601
   - URL completa
   - Status code HTTP
   - TamaÃ±o del contenido
   - SHA256 checksum
   - NÃºmero de intento
   - Resultado (Ã©xito/error)
3. **Checksums SHA256**: ValidaciÃ³n de integridad de cada archivo
4. **Append-only log**: El log nunca se sobrescribe, mantiene histÃ³rico completo
5. **README de metadatos**: DocumentaciÃ³n legible por humanos

### DetecciÃ³n de Cambios
- En ejecuciones posteriores, el sistema:
  1. Lee checksums existentes
  2. Compara con versiÃ³n actual del archivo
  3. Solo re-descarga si hay cambios
  4. Registra en el log si se usÃ³ versiÃ³n cacheada o se re-descargÃ³

### AuditorÃ­a
Todos los archivos de metadatos pueden usarse para:
- Auditar quÃ© datos se usaron en anÃ¡lisis especÃ­ficos
- Verificar integridad de datos
- Reproducir anÃ¡lisis exactos
- Documentar para publicaciones cientÃ­ficas

### ActualizaciÃ³n del Wiki
Para actualizar el contenido del wiki manteniendo trazabilidad:
```bash
# Simplemente ejecuta de nuevo el pipeline
python main.py
# O usa los scripts automÃ¡ticos
./ejecutar_pipeline.sh  # Linux/Mac
ejecutar_pipeline.bat   # Windows
```

El sistema:
1. Detecta quÃ© pÃ¡ginas cambiaron (vÃ­a checksums)
2. Solo re-descarga las modificadas
3. Registra todo en `download_log.jsonl` (append)
4. Actualiza `manifest.json` con nueva fecha
5. Mantiene histÃ³rico completo en el log

## âš ï¸ Consideraciones y Limitaciones

### Rate Limiting
- **Default**: 2 segundos entre requests
- **JustificaciÃ³n**: Scraping responsable, no sobrecargar servidor GitLab
- **Configurable**: Puede ajustarse en `main.py` si es necesario
- **Reintentos**: Backoff exponencial (2, 4, 8 segundos) ante errores

### Dependencias Externas
- **GitLab**: Cambios en la estructura HTML pueden requerir ajustes en selectores
- **Red**: Requiere conexiÃ³n estable para descargas masivas
- **Permisos**: Debe tener acceso al wiki (pÃºblico o credenciales apropiadas)

### Mantenimiento
- **Selectores HTML**: Pueden requerir actualizaciÃ³n si GitLab cambia su UI
- **Logs acumulativos**: `download_log.jsonl` crece con cada ejecuciÃ³n (considerar rotaciÃ³n)
- **Checksums**: Permanecen hasta regeneraciÃ³n completa

### Uso en ProducciÃ³n ClÃ­nica
Este cÃ³digo cumple con prÃ¡cticas de:
- âœ… Reproducibilidad cientÃ­fica
- âœ… Trazabilidad completa
- âœ… ValidaciÃ³n de integridad
- âœ… Logging estructurado
- âœ… Manejo robusto de errores
- âœ… DocumentaciÃ³n exhaustiva

## ğŸ”— Enlaces

- Wiki original: [Datanex Wiki (home)](https://gitlab.com/dsc-clinic/datascope/-/wikis/home)
- Proyecto Datascope: [GitLab - dsc-clinic/datascope](https://gitlab.com/dsc-clinic/datascope)
- GitHub Copilot: [GitHub Copilot](https://github.com/features/copilot)
