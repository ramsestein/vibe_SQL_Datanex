# Pipeline Datanex - Wiki Processor

Pipeline de nivel producci√≥n para descargar, procesar y unificar la wiki de Datanex (base de datos del Hospital Cl√≠nic) con scraping responsable, trazabilidad completa y reproducibilidad cient√≠fica.

> **üìñ Para documentaci√≥n detallada del sistema de scraping**, ver [SCRAPING_GUIDE.md](SCRAPING_GUIDE.md)

## üìã Descripci√≥n

Este proyecto descarga todas las p√°ginas de la wiki de Datanex desde GitLab, las procesa, filtra las p√°ginas relevantes, las convierte a Markdown y genera un archivo unificado (`vibe_SQL_copilot.txt`) que combina un prompt personalizado con toda la documentaci√≥n de la base de datos. Este archivo est√° dise√±ado para ser usado como contexto en GitHub Copilot para generar queries SQL de alta calidad.

## üöÄ Caracter√≠sticas

### Scraping de Nivel Producci√≥n
- **Scraping responsable**: Rate limiting configurable (default: 2s entre requests)
- **Reintentos autom√°ticos**: Backoff exponencial con hasta 3 intentos por p√°gina
- **Validaci√≥n de integridad**: Checksums SHA256 de cada p√°gina descargada
- **Detecci√≥n de cambios**: Solo re-descarga p√°ginas modificadas (ahorra tiempo y ancho de banda)
- **Logging estructurado**: Logs JSON con timestamp, URL, checksums y resultados
- **Metadatos completos**: Manifest, logs y checksums para auditor√≠a y reproducibilidad
- **Manejo robusto de errores**: Contin√∫a ante fallos individuales, registra todo
- **Dominio seguro**: Solo sigue enlaces dentro del wiki (no sale del dominio)
- **User-Agent expl√≠cito**: Identificaci√≥n clara como archivador cl√≠nico/investigaci√≥n

### Pipeline de Procesamiento
- **Descarga autom√°tica**: Descarga recursiva de todas las p√°ginas del wiki de GitLab
- **Estructura jer√°rquica**: Mantiene organizaci√≥n de carpetas reflejando la wiki
- **Filtrado inteligente**: Excluye p√°ginas no relevantes seg√∫n configuraci√≥n
- **Conversi√≥n a Markdown**: HTML ‚Üí Markdown preservando tablas y estructura
- **Limpieza autom√°tica**: Elimina secciones no relevantes y normaliza formato
- **Unificaci√≥n**: Combina todos los documentos en un solo archivo optimizado
- **Procesamiento de diccionarios**: Convierte CSV a Markdown con optimizaci√≥n de tama√±o
- **Compactaci√≥n inteligente**: Reduce tama√±o eliminando prefijos comunes y redundancia
- **Pipeline modular**: Cada paso es independiente, testeable y auditable

## üìÅ Estructura del Proyecto

```
pipeline_datanex/
‚îú‚îÄ‚îÄ src/                          # C√≥digo fuente
‚îÇ   ‚îú‚îÄ‚îÄ download_wiki.py          # Descarga de p√°ginas wiki
‚îÇ   ‚îú‚îÄ‚îÄ extract_text.py           # Extracci√≥n a Markdown
‚îÇ   ‚îú‚îÄ‚îÄ unify_markdown.py         # Unificaci√≥n de markdowns
‚îÇ   ‚îú‚îÄ‚îÄ unify_dictionaries.py     # Unificaci√≥n de diccionarios CSV
‚îÇ   ‚îî‚îÄ‚îÄ create_final_output.py    # Creaci√≥n del archivo final
‚îú‚îÄ‚îÄ test/                         # Tests/Pasos del pipeline
‚îÇ   ‚îú‚îÄ‚îÄ test_download_wiki.py
‚îÇ   ‚îú‚îÄ‚îÄ test_filter_useful_pages.py
‚îÇ   ‚îú‚îÄ‚îÄ test_extract_text.py
‚îÇ   ‚îú‚îÄ‚îÄ test_download_linked_pages.py
‚îÇ   ‚îú‚îÄ‚îÄ test_unify_markdown.py
‚îÇ   ‚îú‚îÄ‚îÄ test_unify_dictionaries.py
‚îÇ   ‚îú‚îÄ‚îÄ test_create_final_output.py
‚îÇ   ‚îî‚îÄ‚îÄ run_all_tests.py          # Ejecuta todo el pipeline
‚îú‚îÄ‚îÄ dicc/                         # Diccionarios CSV
‚îÇ   ‚îú‚îÄ‚îÄ dic_diagnostic.csv        # Diccionario de diagn√≥sticos
‚îÇ   ‚îú‚îÄ‚îÄ dic_lab.csv               # Diccionario de laboratorio
‚îÇ   ‚îî‚îÄ‚îÄ dictionaries_unified.md   # Diccionarios unificados
‚îú‚îÄ‚îÄ data/                         # Datos procesados (ignorado en git)
‚îÇ   ‚îú‚îÄ‚îÄ wiki_html/                # HTML descargado (con estructura jer√°rquica)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metadata/             # Metadatos de descarga (manifest, logs, checksums)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ home.html
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ datanex/              # Subcarpetas seg√∫n estructura del wiki
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ wiki_work_html/           # HTML filtrado (p√°ginas √∫tiles)
‚îÇ   ‚îú‚îÄ‚îÄ wiki_markdown/            # Markdowns generados
‚îÇ   ‚îî‚îÄ‚îÄ wiki_unified.md            # Markdown unificado
‚îú‚îÄ‚îÄ main.py                       # Script principal
‚îú‚îÄ‚îÄ ejecutar_pipeline.bat         # Script batch para ejecutar en Windows
‚îú‚îÄ‚îÄ ejecutar_pipeline.sh          # Script bash para ejecutar en Linux/Mac
‚îú‚îÄ‚îÄ prompt.txt                    # Prompt para Copilot
‚îú‚îÄ‚îÄ pags_descarte.txt             # Lista de p√°ginas a descartar/excluir
‚îî‚îÄ‚îÄ vibe_SQL_copilot.txt          # Archivo final generado
```

## üîß Instalaci√≥n

### Requisitos

- Python 3.8 o superior
- pip

### Pasos

1. **Clonar el repositorio**:
```bash
git clone https://github.com/ramsestein/vibe_SQL_Datanex.git
cd vibe_SQL_Datanex
```

2. **Crear entorno virtual** (recomendado):
```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate
```

3. **Instalar dependencias**:
```bash
pip install -r requirements.txt
```

## üì¶ Dependencias

- `requests` - Para descargar p√°ginas web
- `beautifulsoup4` - Para parsear HTML
- `markdownify` - Para convertir HTML a Markdown

## üéØ Uso

### Ejecutar el pipeline completo

**Windows (m√°s f√°cil)**:
```bash
ejecutar_pipeline.bat
```

**Linux/Mac (m√°s f√°cil)**:
```bash
./ejecutar_pipeline.sh
```

**Desde la l√≠nea de comandos**:
```bash
python main.py
```

Ambos scripts (`ejecutar_pipeline.bat` y `ejecutar_pipeline.sh`) incluyen:
- Creaci√≥n autom√°tica del entorno virtual si no existe
- Verificaci√≥n e instalaci√≥n autom√°tica de dependencias
- Ejecuci√≥n autom√°tica del pipeline completo
- **Push autom√°tico**: Al finalizar el pipeline, sube autom√°ticamente el archivo `vibe_SQL_copilot.txt` al repositorio [vibe_query_DataNex](https://github.com/ramsestein/vibe_query_DataNex)

Este comando ejecuta todos los pasos del pipeline en secuencia:

1. **Descarga desde home**: Descarga la p√°gina "home" de la wiki que contiene el men√∫ lateral con todas las p√°ginas disponibles
2. **Filtrado**: Excluye las p√°ginas listadas en `pags_descarte.txt` (procesa todas las dem√°s)
3. **Extracci√≥n a Markdown**: Convierte las p√°ginas √∫tiles a Markdown
4. **Unificaci√≥n de markdowns**: Combina todos los markdowns de la wiki en un solo archivo (excluyendo las de `pags_descarte.txt`)
5. **Unificaci√≥n de diccionarios**: Convierte diccionarios CSV a Markdown optimizado
6. **Archivo final**: Combina `prompt.txt` + `wiki_unified.md` + `dictionaries_unified.md` ‚Üí `vibe_SQL_copilot.txt`

### Ejecutar pasos individuales

Cada paso puede ejecutarse de forma independiente usando los scripts de test:

```bash
# Paso 1: Descarga
python test/test_download_wiki.py

# Paso 2: Filtrado
python test/test_filter_useful_pages.py

# Paso 3: Extracci√≥n
python test/test_extract_text.py

# Paso 4: Descarga de referencias
python test/test_download_linked_pages.py

# Paso 5: Unificaci√≥n de markdowns
python test/test_unify_markdown.py

# Paso 6: Unificaci√≥n de diccionarios
python test/test_unify_dictionaries.py

# Paso 7: Archivo final
python test/test_create_final_output.py
```

### Ejecutar todos los pasos en secuencia

```bash
python test/run_all_tests.py
```

## ‚öôÔ∏è Configuraci√≥n

### Archivo `pags_descarte.txt`

Este archivo define qu√© p√°ginas de la wiki se **EXCLUIR√ÅN** (descartar√°n) del prompt final. Cada l√≠nea debe contener el nombre de la p√°gina a excluir (sin extensi√≥n):

```
News
FAQs
Access-Instructions
...
```

**‚ö†Ô∏è Importante**: 
- La p√°gina `Overview` **siempre se incluir√°**, incluso si est√° en esta lista de exclusi√≥n.
- Si el archivo est√° vac√≠o o no existe, se procesar√°n **todas** las p√°ginas disponibles.
- Esta es una lista de **exclusi√≥n**, no de inclusi√≥n.
- Las p√°ginas listadas aqu√≠ tambi√©n se excluir√°n autom√°ticamente durante la unificaci√≥n de markdowns.

### Archivo `prompt.txt`

Este archivo contiene el prompt que se incluir√° al inicio del archivo final. Define el comportamiento esperado de Copilot al usar el contexto. Debe contener las secciones `### CONTEXTO ###` y `### DICCIONARIOS ###` donde se insertar√° el contenido correspondiente.

### Carpeta `dicc/`

Contiene los diccionarios CSV que se procesar√°n:
- `dic_diagnostic.csv`: Diccionario de diagn√≥sticos con c√≥digos y descripciones
- `dic_lab.csv`: Diccionario de laboratorio con c√≥digos y descripciones

**‚ö†Ô∏è Configuraci√≥n de columnas**: Para que el sistema procese correctamente los CSV, **debes renombrar manualmente** las columnas en tus archivos CSV para que terminen en `_ref` y `_descr`. Por ejemplo:
- Columna de c√≥digos: `codigo_ref`, `id_ref`, `diagnostic_ref`, etc.
- Columna de descripciones: `descripcion_descr`, `nombre_descr`, `diagnostic_descr`, etc.

El sistema buscar√° autom√°ticamente columnas que terminen en `*_ref` y `*_descr` en todos los archivos CSV de esta carpeta.

Los diccionarios se procesan autom√°ticamente y se optimizan para reducir el tama√±o del archivo final.

## üìä Pipeline Detallado

### Paso 1: Descarga desde home (Scraping Responsable)
- Descarga la p√°gina "home" de la wiki desde [GitLab](https://gitlab.com/dsc-clinic/datascope/-/wikis/home)
- Extrae todos los enlaces del sidebar/men√∫ lateral y contenido principal
- Sigue recursivamente enlaces internos del wiki (no sale del dominio)
- **Rate limiting**: 2 segundos entre requests (configurable)
- **Reintentos**: Hasta 3 intentos con backoff exponencial (2, 4, 8 segundos)
- **Validaci√≥n**: Checksums SHA256, tama√±o m√≠nimo, Content-Type
- **Detecci√≥n de cambios**: Solo re-descarga si el contenido cambi√≥
- Guarda HTML en estructura jer√°rquica: `data/wiki_html/` con subcarpetas
- **Metadatos completos**:
  - `metadata/manifest.json`: Inventario completo (timestamp, URLs, lista de p√°ginas)
  - `metadata/download_log.jsonl`: Log estructurado de cada operaci√≥n (append-only)
  - `metadata/page_checksums.json`: SHA256 de cada p√°gina para detecci√≥n de cambios
  - `metadata/README.md`: Documentaci√≥n de metadatos y reproducibilidad

### Paso 2: Filtrado de p√°ginas √∫tiles
- Lee `pags_descarte.txt` (lista de p√°ginas a EXCLUIR/DESCARTAR)
- Copia TODAS las p√°ginas de `data/wiki_html/` EXCEPTO las listadas en `pags_descarte.txt`
- Siempre incluye `Overview` aunque est√© en la lista de exclusi√≥n
- Guarda las p√°ginas filtradas en `data/wiki_work_html/`

### Paso 3: Extracci√≥n a Markdown de p√°ginas √∫tiles
- Convierte todas las p√°ginas filtradas a Markdown
- Guarda en `data/wiki_markdown/`

### Paso 4: Unificaci√≥n de markdowns
- Combina todos los markdowns en un solo archivo
- **Excluye autom√°ticamente** las p√°ginas listadas en `pags_descarte.txt`
- Elimina secciones no relevantes (como "## Wiki Pages")
- Limpia saltos de l√≠nea dobles
- Convierte tablas HTML a formato Markdown
- Guarda en `data/wiki_unified.md`

### Paso 5: Unificaci√≥n de diccionarios
- Lee todos los archivos CSV de la carpeta `dicc/`
- Busca columnas que terminen en `*_ref` y `*_descr` (deben estar renombradas manualmente)
- Extrae las columnas `*_ref` y `*_descr` de cada CSV
- **Optimizaci√≥n de tama√±o**:
  - Detecta prefijos comunes en los c√≥digos y los compacta
  - Extrae texto com√∫n de las descripciones para evitar repeticiones
  - Para `dic_lab.csv`: elimina conjunciones, determinantes y comas
- Guarda en `dicc/dictionaries_unified.md`

### Paso 6: Archivo final
- Combina `prompt.txt` + `wiki_unified.md` + `dictionaries_unified.md`
- Inserta el contenido de la wiki despu√©s de `### CONTEXTO ###`
- Inserta el contenido de diccionarios despu√©s de `### DICCIONARIOS ###`
- Guarda en `vibe_SQL_copilot.txt`

## üìù Archivos Generados

Durante la ejecuci√≥n del pipeline se generan los siguientes archivos:

### Datos de Descarga
- `data/wiki_html/`: Archivos HTML descargados con estructura jer√°rquica
  - `metadata/manifest.json`: Inventario completo de la descarga
  - `metadata/download_log.jsonl`: Log estructurado (append-only, mantiene hist√≥rico)
  - `metadata/page_checksums.json`: SHA256 checksums para validaci√≥n
  - `metadata/README.md`: Documentaci√≥n de metadatos
  - `home.html`, `datanex/`, etc.: P√°ginas organizadas en carpetas

### Datos Procesados
- `data/wiki_work_html/`: Archivos HTML filtrados (solo p√°ginas √∫tiles)
- `data/wiki_markdown/`: Archivos Markdown generados de cada p√°gina
- `data/wiki_unified.md`: Markdown unificado con todo el contenido de la wiki
- `dicc/dictionaries_unified.md`: Diccionarios CSV convertidos a Markdown

### Salida Final
- `vibe_SQL_copilot.txt`: Archivo final listo para usar en Copilot con estructura:
  ```
  [Contenido de prompt.txt]
  ### CONTEXTO ###
  [Contenido de wiki_unified.md]
  ### DICCIONARIOS ###
  [Contenido de dictionaries_unified.md]
  ```

## üîç Funciones Principales

### `download_wiki_pages()`
Descarga p√°ginas wiki desde GitLab con scraping responsable de nivel producci√≥n:
- Rate limiting configurable (default: 2s)
- Reintentos autom√°ticos con backoff exponencial
- Validaci√≥n de integridad (SHA256 checksums)
- Detecci√≥n de cambios (no re-descarga sin modificaciones)
- Logging estructurado completo
- Metadatos de trazabilidad (manifest, logs, checksums)

### `download_linked_pages()`
Extrae enlaces de archivos Markdown y descarga las p√°ginas referenciadas.

### `filter_useful_pages()`
Filtra p√°ginas excluyendo las que est√°n en `pags_descarte.txt`. Procesa todas las p√°ginas disponibles excepto las listadas. La p√°gina `Overview` siempre se incluye.

### `extract_text()`
Convierte HTML a Markdown preservando tablas y estructura.

### `unify_markdowns()`
Combina m√∫ltiples archivos Markdown en uno solo, limpiando contenido no relevante.

### `unify_dictionaries()`
Convierte diccionarios CSV a Markdown optimizado:
- Detecta prefijos comunes en c√≥digos (sistema de √°rbol)
- Extrae texto com√∫n de descripciones para evitar repeticiones
- Aplica limpieza especial al diccionario de lab (elimina conjunciones, determinantes, comas)
- Formato compacto: `prefix:texto_comun|suffix1:diff1|suffix2:diff2|...`

### `create_final_output()`
Combina el prompt con la documentaci√≥n unificada y los diccionarios, organiz√°ndolos en las secciones `### CONTEXTO ###` y `### DICCIONARIOS ###`.

## üß™ Testing

Los archivos en `test/` act√∫an como pasos individuales del pipeline y pueden ejecutarse de forma independiente para debugging o para ejecutar solo una parte del proceso.

## üìÑ Licencia

Este proyecto es de uso interno para el Hospital Cl√≠nic.

## üî¨ Reproducibilidad y Trazabilidad

Este pipeline est√° dise√±ado para entornos cl√≠nicos y de investigaci√≥n donde la reproducibilidad y trazabilidad son **obligatorias**:

### Reproducibilidad
1. **Determinismo**: Misma entrada ‚Üí misma salida
2. **Versionado completo**: Todo el c√≥digo est√° en Git
3. **Dependencias fijadas**: `requirements.txt` con versiones espec√≠ficas
4. **Metadatos timestamped**: Cada descarga registra fecha, hora y configuraci√≥n
5. **Configuraci√≥n expl√≠cita**: Todos los par√°metros est√°n documentados

### Trazabilidad
1. **Manifest completo** (`manifest.json`): Qu√© se descarg√≥, cu√°ndo, desde d√≥nde
2. **Log estructurado** (`download_log.jsonl`): Cada operaci√≥n registrada con:
   - Timestamp ISO 8601
   - URL completa
   - Status code HTTP
   - Tama√±o del contenido
   - SHA256 checksum
   - N√∫mero de intento
   - Resultado (√©xito/error)
3. **Checksums SHA256**: Validaci√≥n de integridad de cada archivo
4. **Append-only log**: El log nunca se sobrescribe, mantiene hist√≥rico completo
5. **README de metadatos**: Documentaci√≥n legible por humanos

### Detecci√≥n de Cambios
- En ejecuciones posteriores, el sistema:
  1. Lee checksums existentes
  2. Compara con versi√≥n actual del archivo
  3. Solo re-descarga si hay cambios
  4. Registra en el log si se us√≥ versi√≥n cacheada o se re-descarg√≥

### Auditor√≠a
Todos los archivos de metadatos pueden usarse para:
- Auditar qu√© datos se usaron en an√°lisis espec√≠ficos
- Verificar integridad de datos
- Reproducir an√°lisis exactos
- Documentar para publicaciones cient√≠ficas

### Actualizaci√≥n del Wiki
Para actualizar el contenido del wiki manteniendo trazabilidad:
```bash
# Simplemente ejecuta de nuevo el pipeline
python main.py
# O usa los scripts autom√°ticos
./ejecutar_pipeline.sh  # Linux/Mac
ejecutar_pipeline.bat   # Windows
```

El sistema:
1. Detecta qu√© p√°ginas cambiaron (v√≠a checksums)
2. Solo re-descarga las modificadas
3. Registra todo en `download_log.jsonl` (append)
4. Actualiza `manifest.json` con nueva fecha
5. Mantiene hist√≥rico completo en el log

## ‚ö†Ô∏è Consideraciones y Limitaciones

### Rate Limiting
- **Default**: 2 segundos entre requests
- **Justificaci√≥n**: Scraping responsable, no sobrecargar servidor GitLab
- **Configurable**: Puede ajustarse en `main.py` si es necesario
- **Reintentos**: Backoff exponencial (2, 4, 8 segundos) ante errores

### Dependencias Externas
- **GitLab**: Cambios en la estructura HTML pueden requerir ajustes en selectores
- **Red**: Requiere conexi√≥n estable para descargas masivas
- **Permisos**: Debe tener acceso al wiki (p√∫blico o credenciales apropiadas)

### Mantenimiento
- **Selectores HTML**: Pueden requerir actualizaci√≥n si GitLab cambia su UI
- **Logs acumulativos**: `download_log.jsonl` crece con cada ejecuci√≥n (considerar rotaci√≥n)
- **Checksums**: Permanecen hasta regeneraci√≥n completa

### Uso en Producci√≥n Cl√≠nica
Este c√≥digo cumple con pr√°cticas de:
- ‚úÖ Reproducibilidad cient√≠fica
- ‚úÖ Trazabilidad completa
- ‚úÖ Validaci√≥n de integridad
- ‚úÖ Logging estructurado
- ‚úÖ Manejo robusto de errores
- ‚úÖ Documentaci√≥n exhaustiva

## üîó Enlaces

- Wiki original: [Datanex Wiki (home)](https://gitlab.com/dsc-clinic/datascope/-/wikis/home)
- Proyecto Datascope: [GitLab - dsc-clinic/datascope](https://gitlab.com/dsc-clinic/datascope)
- GitHub Copilot: [GitHub Copilot](https://github.com/features/copilot)
